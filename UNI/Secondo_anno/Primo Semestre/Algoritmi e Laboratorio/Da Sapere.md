### **RICORSIONE**
Per applicare la ricorsione devo scomporre il problema in pi√π problemi identici a quello di partenza che siano pi√π piccoli 

### **PROBLEMA DI OTTIMIZZAZIONE**
L'ottimizzazione √® una caratteristica del problema che esiste a prescindere dalla soluzione.
Un problema si dice di ottimizzazione quando questo ha pi√π soluzioni ma non tutte sono uguali e la capacit√† di prendere la soluzione con valore di *bont√†* pi√π alto (o pi√π basso) in base a ci√≤ che chiede il problema stesso

Bont√† = punteggio assegnato alle soluzioni per scegliere poi la migliore

### **ALBERO DI RICORSIONE**
E' un albero dove ogni nodo corrisponde ad una chiamata ricorsiva
![[Pasted image 20251011114749.png]]

**Aspetti negativi della ricorsione**
- La ricorsione occupa memoria per le tante chiamate
- Algoritmo iterativo pi√π veloce a livello pratico perch√© non deve gestire chiamate nella memoria
### **EQUAZIONE DI RICORRENZA**
Descrive il tempo di esecuzione di un algoritmo ricorsivo
$T(n)$ dove $T$ √® il tempo impiegato in base all'input $n$


### **ANALISI DEL COSTO DI UN ALGORITMO**
Analisi asintotica $->$ comportamento dell'algoritmo con valori molto grandi
**Confronto tra funzioni**
1) $F(n) = k | 0<k<‚àû$ **costante**
2) $F(n) = n$ cresce **linearmente** all'input dato
3) $F(n)= log_2\,\,\,n$ cresce meno rispetto alla lineare
4) $F(n) = n\,\,\,log\,\,\,n$ funzione **linearitmica** cresce un po di pi√π della funzione lineare

- funzioni polinomiali
5) $F(n^2)$ funzione **quadratica**
6) $F(n^3)$ funzione **cubica** e cosi via...

- funzione esponenziale
 7) $F(n)= k^n$ 

- Funzione fattoriale
8) $F(n)= n!$
![[Pasted image 20251011121033.png]]


### **Come capire la classe di una funzione**
Se la funzione √® $F(n) = n^3 + 7n^4 + 2n\,\,log^3\,\,n+3$
confrontiamo i termini e da li capiamo quello con la grandezza maggiore:
in questo caso quello che cresce pi√π velocemente √® $7n^4$ quindi l'ordine di grandezza √® $n^4$

Le classi principali sono 3  $ùöØ(g(n))$, $O(g(n))$, $Œ©(g(n))$ 
1) $ùöØ(g(n))$ -> theta di g di n
   √® la classe di tutte le funzioni che hanno un comportamento asintotico simile a g(n),
   $$ùöØ(g(n))= \{F(n):∆é\,\, c_1,c_2,n_0| 0‚â§ c_1\,g(n)‚â§F(n)‚â§ c_2\,g(n)‚àÄn ‚â• n_0  \}$$
![[Pasted image 20251011154542.png|300]]
$F(n)$ sta sempre in mezzo o al massimo uguale a $c_1\,g(n)$ e $c_2\,g(n)$

2)  $O(g(n))$ 
   √® la classe che limita superiormente il comportamento di una funzione: ovvero la funzione non andr√† mai sopra $O(n)$
   $$O(g(n)) = \{F(n):∆é\,\, c,n_0| 0‚â§ F(n)‚â§c\,g(n)‚àÄn ‚â• n_0\}$$ ![[Pasted image 20251011155511.png|300]]
$F(n)$ sta sempre sotto o al massimo uguale alla funzione $O(g(n))$

3) $Œ©(g(n))$
   √® la classe che limita inferiormente il comportamento di una funzione: ovvero la funzione non andr√† mai sotto $Œ©(g(n))$
   $$Œ©(g(n)) = \{F(n):∆é\,\, c,n_0| 0‚â§ c\,g(n)‚â§F(n)‚àÄn ‚â• n_0\}$$   
![[Pasted image 20251011160111.png|300]]
$F(n)$ sta sempre sopra o al massimo uguale alla funzione $Œ©(g(n))$

esiste anche $o$ piccolo ed √® uguale a $O$ solo che la funzione si comporta sempre meglio e mai uguale

### **A partire dalla procedura posso capire la classe?**
(si) da capire e fare


### **Risoluzione equazioni di ricorrenza**
Se indichiamo con T(n) il costo dell‚Äôalgoritmo su un problema di dimensione n, possiamo esprimere questo comportamento nel modo seguente:
$$T(n) = aT(\frac{n}{b}) + f(n)$$Dove a rappresenta il numero di sottoproblemi prodotti, b il fattore di riduzione della dimensione e f (n) il costo complessivo delle operazioni non ricorsive, ossia di tutte quelle attivit√† che non comportano nuove chiamate, come la divisione del problema, la fusione dei risultati o eventuali operazioni preliminari

Non tutti gli algoritmi ricorsivi possono essere descritti mediante l' equazione soprastante:
(es. quickSort)
Questi esempi mostrano che, quando la riduzione del problema non √® uniforme, non √® possibile applicare direttamente formule standard come il teorema Master, e occorre analizzare la ricorrenza caso per caso. Tuttavia, i principi generali rimangono gli stessi: √® sempre possibile visualizzare la struttura delle chiamate come un albero, contare i nodi generati e sommare i costi dei livelli per ottenere il comportamento complessivo dell‚Äôalgoritmo

#### **Il metodo dell'albero di ricorsione**
L'idea √® costruire un albero in cui ogni nodo rappresenta una singola invocazione dell'algoritmo ed ogni arco corrisponda ad una chiamata ricorsiva. In questo modo possiamo capire passo dopo passo come si distribuisce il lavoro complessivo nei diversi livelli di ricorsione
Immaginiamo di avere un algoritmo descritto da: $$T(n) = aT(\frac{n}{b}) + f(n)$$Il metodo dell‚Äôalbero di ricorsione consiste nel rappresentare questa relazione sotto forma di un albero in cui:
‚Ä¢ la *radice* dell‚Äôalbero corrisponde al problema iniziale di dimensione $n$; 
‚Ä¢ i *nodi interni* rappresentano le chiamate ricorsive generate a ciascun livello; 
‚Ä¢ ogni nodo √® etichettato con il costo del lavoro locale f ($n_i$), dove $n_i$ √® la dimensione del sottoproblema corrispondente; 
‚Ä¢ i *figli di un nodo* rappresentano le chiamate generate da quel nodo, ognuna di dimensione ridotta di un fattore $b$.

La costruzione di questo albero permette di scomporre la ricorsione in livelli. Il primo livello contiene un solo nodo (il problema originale), il secondo livello contiene $a$ nodi (uno per ciascun sottoproblema), il terzo livello ne contiene $a_2$, e cos√¨ via. A ogni livello $k$ dell‚Äôalbero il numero di nodi √® $a^k$, e la dimensione dei sottoproblemi √® ridotta a $\frac{n}{b^k}$. Il costo totale associato al livello $k$ pu√≤ quindi essere scritto come: $$C^k = a^k \,f(\frac{n}{b^k})$$cio√® come il numero di nodi di quel livello moltiplicato per il costo del lavoro svolto in ciascun nodo.
L'idea √® quella di fare la somma dei costi dei singoli livelli dell'albero
![[Pasted image 20251024110126.png|400]]
dove:
- $L$ rappresenta la profondit√† dell'albero
Questo approccio fornisce non solo un modo intuitivo per stimare la crescita di T (n), ma anche una rappresentazione visiva dell‚Äôandamento del lavoro. Osservando come varia il costo dei livelli successivi, √® possibile capire se il lavoro complessivo √® dominato dai livelli pi√π alti (quelli vicini alla radice), dai livelli intermedi o da quelli pi√π profondi dell‚Äôalbero

Abbiamo 3 possibili casistiche che corrispondono ai 3 casi del teorema master
1) Quando la somma √® dominata dai primi livelli, il costo totale √® determinato dal lavoro iniziale $f (n)$
2) Quando tutti i livelli hanno lo stesso ordine di grandezza, il costo cresce come il numero dei livelli
3) Quando i livelli inferiori diventano progressivamente pi√π costosi, il termine dominante si sposta verso il fondo dell‚Äôalbero

**COME APPLICARE QUESTO METODO**
**Esempio ricerca binaria**
Consideriamo l'equazione:
$$T(n) = T(\frac{n}{2})+1$$
Ogni chiamata ricorsiva genera una sola chiamata su met√† dell'input, e il lavoro aggiuntivo (calcolo dell'indice medio e confronto) √® costante. Possiamo immaginare l'albero della ricorsione come una catena di chiamate successive, in cui ogni nodo produce un unico figlio di dimensione dimezzata, √® una dimensione che ogni livello si dimezza quindi la profondit√† dell'albero √® pari a $log_2 \,\,n$. Il costo totale si ottiene sommando il lavoro di tutti i livelli
Albero: 
![[Pasted image 20251024113845.png|30]]
$$T (n) = 1 + 1 + 1 + ¬∑ ¬∑ ¬∑ + 1$$
dove il numero dei termini 1 √® pari a $log_2 \,\,n+ 1$. Da qui risulta immediatamente che la complessit√† √® $T(n) = O(log n)$
In questo caso, il costo per livello √® costante e l‚Äôalbero ha profondit√† logaritmica: il risultato √® quindi una crescita proporzionale al numero dei livelli







#### Il metodo della sostituzione
Consiste nel formulare un‚Äôipotesi sulla forma asintotica della soluzione e nel dimostrare che tale ipotesi √® corretta attraverso un ragionamento induttivo.
Si parte dall‚Äô equazione di ricorrenza e, osservando la struttura del problema, si tenta di ‚Äúindovinare‚Äù la crescita di T (n), ad esempio di O(n) O($n^2$) ecc... . Una volta formulata una ipotesi la si sostituisce nell'equazione e si verifica se l'uguaglianza (o disuguaglianza) risulta soddisfatta per valori sufficientemente grandi di n. Se l‚Äôipotesi risulta coerente, viene cos√¨ confermata; altrimenti, la si modifica finch√© non produce una forma valida.
Questo metodo √® chiamato ‚Äúdella sostituzione‚Äù perch√© prevede di sostituire l‚Äôipotesi nella ricorrenza, semplificarla e controllare che il risultato sia coerente.

Si hanno quindi 3 fasi da seguire:
1) Si formula un‚Äôipotesi sul comportamento asintotico di T (n). Spesso l‚Äôipotesi deriva dall'intuizione fornita da metodi pi√π intuitivi come l‚Äôalbero di ricorsione 
2) Si sostituisce questa ipotesi all‚Äôinterno dell‚Äôequazione di ricorrenza e si verifica se l‚Äôuguaglianza o disuguaglianza risulta soddisfatta
3) Se necessario, si aggiusta l‚Äôipotesi (ad esempio includendo una costante o un termine logaritmico) fino a ottenere un‚Äôespressione coerente

**COME APPLICARE QUESTO METODO**
**Esempio ricerca binaria**
Consideriamo l'equazione:
$$T(n) = T(\frac{n}{2})+1$$
L'intuizione suggerisce una crescita logaritmica. Supponiamo quindi $T(n)‚â§ c \,log_2 \,\,n$, sostituendolo nell'equazione otteniamo: 
![[Pasted image 20251024135108.png]]
Affinch√© la disuguaglianza $T(n)‚â§ c \,log_2\,\,n$ sia rispettata, √® sufficiente che $-c+1‚â§ 0$ cio√® $c‚â•1$ Anche in questo caso la nostra ipotesi √® coerente $T(n) = O(log\,\,n)$

#### TEOREMA MASTER










### **Strutture dati**
**HEAP** 
Per implementare una struttura dati astratta : Coda con priorit√† (lifo) 
- A parit√† di valore si considera il tempo di arrivo
- Si considera la chiave (valore pi√π piccola)
![[Pasted image 20251020180019.png]]
le chiavi sono i valori dentro l'array, mai la posizione

Ovviamente per essere efficiente bisogna che abbia un costo basso

![[Pasted image 20251020180140.png|500]]

1) array disordinato
2) array ordinato
3) BST

**Definizione HEAP**
Struttura dati non lineare. E' un albero binario dove ogni nodo ha al pi√π due figli. √à posizionale, ha un figlio sinistro e uno destro, a sinistra √® pi√π piccolo della clave a destra pi√π grade. √à completo. Tutti i livelli sono pieni. L'ultimo livello pu√≤ essere non pieno ma solo se i nodi sono allineati da sinistra verso destra![[Pasted image 20251020180427.png|400]]
![[Pasted image 20251020180450.png]]

